{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измалкова Дарья БКЛ182\n",
    "\n",
    "# Домашнее задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "stops = set(stopwords.words('russian'))\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция, которая приводит текст к нижнему регистру и лемматизирует его. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_and_lemm(text):\n",
    "    lemmas = ''.join(m.lemmatize(text))\n",
    "    clean_text = [w.lower() for w in word_tokenize(lemmas) if w.isalpha() and w not in stops]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция, возвращающая список не супер редких слов из списка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators(texts):\n",
    "    indics = set()\n",
    "    word_count = Counter()\n",
    "    for text in texts:\n",
    "        word_count = word_count + Counter(token_and_lemm(text))\n",
    "        \n",
    "\n",
    "    for key, value in word_count.items():\n",
    "        if value  > 2:\n",
    "            indics.add(key)\n",
    "    return indics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция, которая находит отличительные слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segregate(pos, neg):\n",
    "    pos_count = get_indicators(pos)\n",
    "    neg_count = get_indicators(neg)\n",
    "\n",
    "    pos_words = pos_count - neg_count\n",
    "    neg_words = neg_count - pos_count\n",
    "    return pos_words, neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция, которая оценивает текст "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(pos_ind, neg_ind, text):\n",
    "    count = Counter()\n",
    "    for word in token_and_lemm(text):\n",
    "        if word in pos_ind:\n",
    "            count['pos'] += 1\n",
    "        elif word in neg_ind:\n",
    "            count['neg'] += 1\n",
    "\n",
    "    if count['pos'] > count['neg']:\n",
    "        \n",
    "        return 'positive'\n",
    "    elif count['pos'] < count['neg']:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'undecided'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сбор данных: \n",
    "взяла отзывы о фильме \"Довод\" с www.afisha.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(page):\n",
    "    html = requests.get(page)\n",
    "    soup = BeautifulSoup(html.text,'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 'https://www.afisha.ru/movie/258600/reviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(page):\n",
    "    data = []\n",
    "    \n",
    "    page_count = 1\n",
    "    adds = ['positive/','negative/']\n",
    "    \n",
    "    for add in adds:\n",
    "        count = 0\n",
    "        \n",
    "        if 'positive' in add:\n",
    "                value = 'positive'\n",
    "        elif 'negative' in  add:\n",
    "                value = 'negative'\n",
    "                \n",
    "        soup = make_soup(page + add)       \n",
    "        for r in soup.find_all(itemprop = 'review'):\n",
    "            review = str(r.find(itemprop = 'reviewBody'))\n",
    "# .text / .get_text() почему-то выдавали пустоту\n",
    "    \n",
    "            count += 1\n",
    "            \n",
    "            if count == 25:\n",
    "                add2 = add + 'page2/'\n",
    "                soup = make_soup(page + add2)       \n",
    "                for r in soup.find_all(itemprop = 'review'):\n",
    "                    review = str(r.find(itemprop = 'reviewBody'))\n",
    "                    data.append({'text' : review, 'rating' : value})\n",
    "\n",
    "            data.append({'text' : review, 'rating' : value})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Делю отзывы на список позитивных и негативных и отбираю 10 на тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "pos_data = []\n",
    "neg_data = []\n",
    "for item in data:\n",
    "    \n",
    "    if item['rating'] == 'positive':\n",
    "        if len(pos_data) < 45:\n",
    "            pos_data.append(item['text'])\n",
    "        else:\n",
    "                test_data.append(item)\n",
    "    elif item['rating'] == 'negative':\n",
    "            if len(neg_data) < 45:\n",
    "                neg_data.append(item['text'])\n",
    "            else:\n",
    "                test_data.append(item)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Собираю списки отличительных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ind, neg_ind = segregate(pos_data,neg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "reality = []\n",
    "for item in test_data: \n",
    "    predictions.append(assess(pos_ind, neg_ind, item['text']))\n",
    "    reality.append(item['rating'])\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(predictions, reality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как улучшить программу? \n",
    "<br>\n",
    "\n",
    " <li> 1) Собрать выборку побольше </li>\n",
    " <li> 2) Попробовать с нейросетью, чтобы не полагаться на конкретные тексты и вхождения в них</li>\n",
    " <li> 3) Возможно есть способ оценить положительность/отрицательность не только на лексике</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
